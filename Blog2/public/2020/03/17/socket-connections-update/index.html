<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.68.3" />


<title>Socket Connections Update - The R Blog</title>
<meta property="og:title" content="Socket Connections Update - The R Blog">




  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">

<link rel="icon" type="image/png"
      href="/images/favicon-32x32.png"
      sizes="32x32" />

<link rel="icon" type="image/png"
      href="/images/favicon-16x16.png"
      sizes="16x16" />



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/Rlogo.png"
         width="100"
         height="78"
         alt="R">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/index.html">About</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">


    
      <h1 class="article-title">Socket Connections Update</h1>
            
        <h2 class="article-author">Tomas Kalibera, Luke Tierney</h2>
      
      
      
        <span class="article-metadata">Categories:
        User-visible Behavior, Internals
        </span>
        <br>
      
      
        <span class="article-metadata">Tags:
        sockets, parallel, PSOCK cluster
        </span>
        <br>
            
      
      <span class="article-date">First published: 2020/03/17</span>
    

    <div class="article-content">
      


<p>Starting up a PSOCK cluster is not fast. In R 3.6 on just a few years old
laptop with 8 logical cores, running Windows, it takes about 1.7s to start a
cluster with 8 nodes:</p>
<pre><code>library(parallel); system.time(cl &lt;- makePSOCKcluster(8))</code></pre>
<p>A good design is to start a cluster only once during an R session and then
pass it to computations that can take advantage of it. This is needed so
that the end user always has full control over how many cores are used in
total. Starting a cluster in package code out of direct control of the user
often causes big slowdowns by overloading the machine, resulting in much
worse performance than sequential execution.</p>
<p>The 1.7s thus may seen acceptable, but if we start a larger cluster on a
server machine with many cores, one node per logical core, the startup times
become prohibitively large. On a recent Fedora server with 64 logical
cores, it takes about 14s. On an old Solaris server with 64 logical cores,
it takes 211s!</p>
<p>In R-devel, we have extended the sockets API and re-designed the startup of
a PSOCK cluster. The Windows laptop mentioned above now starts the cluster in
0.5s in R-devel. Timings for several other machines with more cores are</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">R 3.6</th>
<th align="right">R-devel</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Fedora server (64 cores)</td>
<td align="right">14s</td>
<td align="right">0.4s</td>
</tr>
<tr class="even">
<td align="left">Ubuntu server (40 cores)</td>
<td align="right">6.6s</td>
<td align="right">0.4s</td>
</tr>
<tr class="odd">
<td align="left">Windows server (48 cores)</td>
<td align="right">9.3s</td>
<td align="right">0.5s</td>
</tr>
<tr class="even">
<td align="left">Solaris server (64 cores)</td>
<td align="right">211s</td>
<td align="right">7s</td>
</tr>
<tr class="odd">
<td align="left">macOS desktop (12 cores)</td>
<td align="right">4.2s</td>
<td align="right">0.7s</td>
</tr>
</tbody>
</table>
<p>The speedup is large when the number of cores is large, but the
individual cores are slow.</p>
<div id="compatibility" class="section level1">
<h1>Compatibility</h1>
<p>The rest of this post describes in detail how these performace improvements
were achieved.
The socket layer improvements do not change documented behavior for the
existing API, but change observable behavior (sometimes timeouts are
enforced when they were not before, <code>blocking = FALSE</code> is respected on the
server end of a connection). The usual way of testing R changes via
comparing results of package checks does not help too much here as the CRAN
policies limit the number of cores to 2 for the checks (to prevent
overloading of the check machines). Users relying on parallelization/sockets
communication are hence invited and encouraged to run any of their tests
they have on large systems and report any new bugs.</p>
</div>
<div id="starting-a-psock-cluster-in-r-3.6" class="section level1">
<h1>Starting a PSOCK cluster in R 3.6</h1>
<p>R 3.6 and earlier starts a cluster sequentially. For each node, the server
issues a <code>system()</code> command to start the node, then waits via
<code>socketConnection(server = TRUE)</code> for the node to connect, and then does the
same for the second node, etc. Nearly all the time to start a cluster is
spent in starting all the R sessions.</p>
<p>Even though simple on the server, it is not simple in the nodes: when a node
is started, it tries to connect to the server via <code>socketConnection(server = FALSE)</code>, but the connection may fail due to a race condition: the operating
system may decide to run <code>socketConnection</code> in the node before
<code>socketConnection</code> in the server, and then the connection will fail.
Therefore, even in R 3.6 cluster setup, the nodes had to re-try in case of a
failure, and did so with an exponential backoff.</p>
<p>There is no way to avoid this race condition, because
<code>socketConnection(server = TRUE)</code> does all three of the server socket
operations: <code>bind()</code>, <code>listen()</code>, and <code>accept()</code>. It always creates and
binds a temporary server socket, makes it listening, waits for a connection
via <code>accept()</code>, and then destroys the server socket. In the time intervals
when there is no listening server socket on the given port on the server,
the connection from the node will fail.</p>
<p>In practice, this has been working well, because on typical operating
systems the server will soon be scheduled and the number of retries
will be small. However, if we were starting the nodes in parallel,
the number of retries would probably increase dramatically with the
increasing number of nodes, which would damage performance.</p>
</div>
<div id="server-socket-connections" class="section level1">
<h1>Server socket connections</h1>
<p>We have, instead, extended the R connections API so that one can work with
server socket connections directly, re-using them for accepting multiple
socket connections:</p>
<pre><code>serverSocket(port)
     
socketAccept(socket, blocking = FALSE, open = &quot;a+&quot;,
             encoding = getOption(&quot;encoding&quot;),
             timeout = getOption(&quot;timeout&quot;))</code></pre>
<p><code>serverSocket</code> creates a listening server socket connection, which is of a
new class <code>"servsockconn"</code>. <code>socketAccept</code> accepts an incoming connection
to the given server socket. <code>socketSelect</code> can be used on a listening
server socket to wait for when a connection is ready to be accepted. The
server socket connection can be closed by <code>close</code> as usual.</p>
</div>
<div id="listen-backlog" class="section level1">
<h1>Listen backlog</h1>
<p>The operating system has a queue of partial and established incoming
connections. Established connections are ready to be given to the
application via <code>accept()</code> (<code>socketAccept()</code>). This queue has a limited
length, which one can influence via an argument to <code>listen()</code>, but it is
only a hint and one cannot programmatically find out how large the queue
really is. The operating system may impose a limit and make that queue
shorter. We have modified R-devel to use the maximum length supported on
each system, but there is no guarantee it would be enough for all nodes.
The cluster setup hence needs to be behave correctly when the queue is too
short.</p>
<p>TCP uses a 3-way handshake when establishing a connection. In the normal
successful case when the queue is not full, a node sends <code>SYN</code> packet, the
server puts the connection into a queue and responds with <code>SYN+ACK</code>, the
node then gives the connection to the application as established (via
<code>connect()</code>/<code>socketConnection(server=FALSE)</code>) and sends <code>ACK</code> to the server.
The server then flags the connection as established (some systems put it to
a different queue) and gives it to the application (via
<code>accept()</code>/<code>socketAccept()</code>).</p>
<p>When the queue is full, a number of things can happen. Linux has
effectively two queues, one for already established connections and one for
partially established ones. Only the size of the established queue is
influenced by the backlog argument to <code>listen()</code>, and when that queue is full,
Linux already decreases the rate of adding connections to the
partially-established queue. It may simply drop a <code>SYN</code> packet and not put
an incoming connection to the partially-established queue. The TCP layer
on the node will retry a few times, re-sending the <code>SYN</code>, but eventually give
up, send a <code>RST</code> (reset), and the connection will fail. It is hence
necessary to keep the code for retries after failure in the nodes even with
the new server socket API.</p>
<p>Moreover, it can still happen even on Linux that an <code>ACK</code> from the node is
received on the server, but the established queue is full. Then the server
may send a <code>RST</code> to the node and the node will fail, because it is already
blocked waiting for commands from the server on a connection it believes is
already established. A similar situation arises when the server drops the
<code>ACK</code> packet, but keeps the connection in the partially-established queue.
It may then after a timeout re-send <code>SYN+ACK</code> to the node, the node re-sends
its <code>ACK</code>, and eventually the connection may really succeed on the server or
the server may send a <code>RST</code> and remove it from the partially-established
queue. More information on how Linux implements the backlog is available
<a href="http://veithen.io/2014/01/01/how-tcp-backlog-works-in-linux.html">here</a>.</p>
<p>An additional complication is that the server may drop the <code>ACK</code> from the
node and remove the connection from the partially-established queue.
Empirically we have seen this during a stress test when the connections from
the client were old, and the number of occurrences increased with the age of
the connections, so presumably this is after a timeout. Consequently, the
node then does not receive a <code>RST</code> and will keep waiting for a command from
the server indefinitely.</p>
<p>This situation is known as half-opened connection and can arise in various
ways in TCP communication. It would be resolved by the TCP layer if the
node started communicating over the connection, but in the PSOCK protocol,
it is the server that starts communicating, so this issue needs to be
handled specially.</p>
</div>
<div id="server-initiated-handshake" class="section level1">
<h1>Server-initiated handshake</h1>
<p>We have thus changed the cluster setup procedure so that the server, as soon
as it gets a connection from the node, sends an initial command to the node
as a handshake. The node waits for such initial command during the setup
phase and if it does not get the command in some time (half-opened
connection closed on server) or if the waiting fails (it gets a <code>RST</code> from
the server), the node re-tries establishing a connection to the server and
waiting for a new handshake. This does not change the wire protocol: the
handshake is just a regular command and the node runs it and sends a
response.</p>
</div>
<div id="connection-timeouts" class="section level1">
<h1>Connection timeouts</h1>
<p>The R 3.6 sockets API allows to define a timeout for a socket connection at
creation time. The timeout then influences most operations on the socket
(applies individually to low-level operations, but the R-level functions may
wait somewhat longer in total). PSOCK clusters use connection timeouts of
30 days: if there is no command from the server (e.g. the server crashes or
connection is lost) to the node within that time, the node exits on its own.
For the handshake during cluster setup, we would need a much shorter
timeout. We have hence extended the API to allow modifying a timeout of a
socket connection:</p>
<pre><code>socketTimeout(socket, timeout = -1)</code></pre>
<p>This new function allows using a short timeout during the handshake and in
<code>socketConnection</code> invocation on the node, which was previously
effectively blocking (blocking on Linux due to a select() bug, timeout of 30
days on other systems). After the handshake, <code>socketTimeout</code> is called to
increase the timeout again to 30 days.</p>
<p>The server initiated handshake in addition to helping with half-opened
connections opened on the client only (via the timeout) also gets rid of any
half-opened connection opened only on the server. The TCP layer will detect
those and fail when the server starts communicating. We have observed these
as well during our initial stress testing.</p>
</div>
<div id="parallel-psock-cluster-setup" class="section level1">
<h1>Parallel PSOCK cluster setup</h1>
<p>In R-devel, the new parallel cluster setup is used by default for
homogeneous clusters with all nodes running on <code>localhost</code>, when all nodes are
started automatically. The original sequential startup code is still
supported for the other cases. There is a new cluster option
<code>setup_strategy</code> with values <code>"parallel"</code> and <code>"sequential"</code>. <code>"parallel"</code>
is the default and tells R to use parallel strategy on all cluster where
supported.</p>
</div>
<div id="r-socket-layer-improvements" class="section level1">
<h1>R socket layer improvements</h1>
<p>Several issues of the R socket layer implementation were fixed as part of
this work.</p>
<p>The connection timeout with <code>socketConnection(server = FALSE)</code> on Linux is
now enforced. Before, the call was accidentally blocking due to
Linux-specific behavior of <code>select()</code>.</p>
<p><code>socketConnection(server = FALSE)</code> on Windows now returns immediately when
the connection fails. Before, one had to wait for a timeout to expire due
to Windows-specific behavior of <code>select()</code> when waiting for a connection.</p>
<p><code>socketConnection(server = FALSE)</code> now detects when a connection is
available right away without waiting (probably unlikely and only possible on
a <code>localhost</code>) and returns it. Previously, R would wait anyway and attempt to
connect again, possibly leaking the connection.</p>
<p><code>socketConnection(server = TRUE)</code> (and <code>socketAccept()</code>) now enforce the
connection timeout. Previously, they could block indefinitely due to a race
condition when a connection seems available to be accepted by <code>select()</code>,
but is re-set by the client by the time <code>accept()</code> is called. On some
systems, <code>accept()</code> would then block. On other (we triggered this on Solaris),
<code>accept()</code> would then fail; after the change, R will keep waiting for a good
connection respecting the timeout.</p>
<p>Read operation from a socket is now robust against spurious readability of
the socket (<code>select()</code> reports it as readable, but then e.g. due to an
invalid checksum <code>recv()</code> would block). This problem may happen on Linux.</p>
<p>Write operation to a socket is now robust against spurious writeability of
the socket. Previously, this case could lead to unpredictable behavior.
However, this kind of <code>select()</code> bug has not been reported on any system to
our knowledge.</p>
<p>The <code>blocking = FALSE</code> argument on a socket connection (<code>socketConnection()</code>) is
now respected also on the server side of a socket. Previously, read/write
operations on the server end of a socket were accidentally blocking even
with <code>blocking = FALSE</code>.</p>
<p>The internal handling of status codes from socket operations on Windows has
been updated for WinSock2.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    

    
  </body>
</html>

